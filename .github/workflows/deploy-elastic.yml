name: Deploy Elasticsearch Stack

on:
  push:
    branches: [ main ]
    paths:
      - 'elasticsearch-production/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'elasticsearch-production/**'

env:
  HELM_VERSION: v3.12.0
  KUBECTL_VERSION: v1.27.0
  KUBECONFIG: ${{ github.workspace }}/.kube/config

jobs:
  validate-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}

      - name: Helm lint operator chart
        run: |
          cd elasticsearch-production/elastic-operator
          helm dependency update
          helm lint .

      - name: Helm lint stack chart
        run: |
          cd elasticsearch-production/elastic-stack
          helm dependency update
          helm lint .

      - name: Package Helm charts
        run: |
          mkdir -p charts
          cd elasticsearch-production/elastic-operator
          helm package . -d ../../charts
          cd ../elastic-stack
          helm package . -d ../../charts

      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG }}
        
      - name: Deploy ECK operator
        run: |
          cd elasticsearch-production/elastic-operator
          helm upgrade --install elastic-operator . \
            --namespace elastic-system \
            --create-namespace \
            --wait \
            --timeout 2m

      - name: Wait for operator to be ready
        run: |
          kubectl wait --for=condition=available deployment/elastic-operator-elastic-operator \
            --namespace elastic-system \
            --timeout=300s

      - name: Deploy Elasticsearch stack
        run: |
          cd elasticsearch-production/elastic-stack
          helm upgrade --install elastic-stack . \
            --namespace elastic-system \
            --wait \
            --timeout 3m

      - name: Health check - Elasticsearch
        run: |
          # Wait for Elasticsearch pods to be ready
          kubectl wait --for=condition=ready pod \
            -l common.k8s.elastic.co/type=elasticsearch \
            --namespace elastic-system \
            --timeout=600s
          
          # Get Elasticsearch health
          ES_PASSWORD=$(kubectl get secret -n elastic-system elasticsearch-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)
          ES_POD=$(kubectl get pods -n elastic-system -l common.k8s.elastic.co/type=elasticsearch -o jsonpath='{.items[0].metadata.name}')
          
          # Check cluster health
          kubectl exec -n elastic-system $ES_POD -- curl -u "elastic:$ES_PASSWORD" -s -k \
            https://localhost:9200/_cluster/health | grep -q '"status":"green"'

      - name: Health check - Kibana
        run: |
          # Wait for Kibana pods to be ready
          kubectl wait --for=condition=ready pod \
            -l common.k8s.elastic.co/type=kibana \
            --namespace elastic-system \
            --timeout=300s
          
          # Check Kibana status
          KIBANA_POD=$(kubectl get pods -n elastic-system -l common.k8s.elastic.co/type=kibana -o jsonpath='{.items[0].metadata.name}')
          kubectl logs $KIBANA_POD -n elastic-system | grep -q "Kibana is now available"

      - name: Health check - Filebeat
        run: |
          # Wait for Filebeat pods to be ready
          kubectl wait --for=condition=ready pod \
            -l common.k8s.elastic.co/type=beat \
            --namespace elastic-system \
            --timeout=300s
          
          # Check if Filebeat is sending data
          FB_POD=$(kubectl get pods -n elastic-system -l common.k8s.elastic.co/type=beat -o jsonpath='{.items[0].metadata.name}')
          kubectl logs $FB_POD -n elastic-system | grep -q "Connection to backoff(elasticsearch"

      - name: Notify on failure
        if: failure()
        run: |
          echo "Deployment or health checks failed. Check the logs for more details."
          exit 1
